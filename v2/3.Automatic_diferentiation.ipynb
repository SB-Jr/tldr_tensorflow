{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "\n",
    "Once we start building models we may sometimes need to create new operations or functions to take care of new types of blocks in the neural network and handle the loss propagation ourselves. This is also helpful when we make some changes in our model and we need full control on how the model will be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.GradientTape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the gradient we use the `GradientTape` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:24:22.568994Z",
     "iopub.status.busy": "2020-11-03T14:24:22.568405Z",
     "iopub.status.idle": "2020-11-03T14:24:24.758713Z",
     "shell.execute_reply": "2020-11-03T14:24:24.758217Z",
     "shell.execute_reply.started": "2020-11-03T14:24:22.568923Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:27:14.746387Z",
     "iopub.status.busy": "2020-11-03T14:27:14.745457Z",
     "iopub.status.idle": "2020-11-03T14:27:17.239736Z",
     "shell.execute_reply": "2020-11-03T14:27:17.237466Z",
     "shell.execute_reply.started": "2020-11-03T14:27:14.746250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df/dx where f=(x)^2: \n",
      " None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n",
    "    f = x ** 2\n",
    "\n",
    "print('The df/dx where f=(x)^2: \\n', grad.gradient(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that if x is a constant the gradient doesnt come out to be 0 but it comes as None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:28:19.210572Z",
     "iopub.status.busy": "2020-11-03T14:28:19.209919Z",
     "iopub.status.idle": "2020-11-03T14:28:19.289782Z",
     "shell.execute_reply": "2020-11-03T14:28:19.287103Z",
     "shell.execute_reply.started": "2020-11-03T14:28:19.210498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df/dx where f=(x)^2: \n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n",
    "    f = x ** 2\n",
    "\n",
    "print('The df/dx where f=(x)^2: \\n', grad.gradient(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the recommended way is to use `.watch()` operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:29:45.891538Z",
     "iopub.status.busy": "2020-11-03T14:29:45.890875Z",
     "iopub.status.idle": "2020-11-03T14:29:45.914994Z",
     "shell.execute_reply": "2020-11-03T14:29:45.912503Z",
     "shell.execute_reply.started": "2020-11-03T14:29:45.891449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df/dx where f=(x)^2: \n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n",
    "    grad.watch(x)\n",
    "    f = x ** 2\n",
    "\n",
    "print('The df/dx where f=(x)^2: \\n', grad.gradient(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:30:06.499646Z",
     "iopub.status.busy": "2020-11-03T14:30:06.498995Z",
     "iopub.status.idle": "2020-11-03T14:30:06.524736Z",
     "shell.execute_reply": "2020-11-03T14:30:06.522883Z",
     "shell.execute_reply.started": "2020-11-03T14:30:06.499572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df/dx where f=(x)^2: \n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n",
    "    grad.watch(x)\n",
    "    f = x ** 2\n",
    "\n",
    "print('The df/dx where f=(x)^2: \\n', grad.gradient(f, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Variable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:42:32.932961Z",
     "iopub.status.busy": "2020-11-03T14:42:32.931981Z",
     "iopub.status.idle": "2020-11-03T14:42:32.984592Z",
     "shell.execute_reply": "2020-11-03T14:42:32.982208Z",
     "shell.execute_reply.started": "2020-11-03T14:42:32.932843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:  tf.Tensor([[22. 28.]], shape=(1, 2), dtype=float32)\n",
      "Loss:  tf.Tensor(634.0, shape=(), dtype=float32)\n",
      "d(Loss)/dw:  tf.Tensor(\n",
      "[[22. 28.]\n",
      " [44. 56.]\n",
      " [66. 84.]], shape=(3, 2), dtype=float32)\n",
      "d(Loss)/db:  tf.Tensor([22. 28.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([[1, 2],\n",
    "                 [3, 4],\n",
    "                 [5, 6]], dtype=tf.float32, name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1, 2, 3]]\n",
    "\n",
    "with tf.GradientTape(persistent=False) as grad:\n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y ** 2)\n",
    "    [dl_dw, dl_db] = grad.gradient(loss, [w, b])\n",
    "\n",
    "print('Y: ', y)\n",
    "print('Loss: ', loss)\n",
    "print('d(Loss)/dw: ', dl_dw)\n",
    "print('d(Loss)/db: ', dl_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent Gradient\n",
    "\n",
    "By default the resources are released once we call the grad.gradient() function. But if we have to calculate multiple functions over the same variable then we will need to persist the gradient so that the functions and its relations are preserved even once we have called grad.gradient().\n",
    "\n",
    "**Note: Make sure to use `del grad` once done so that the garbage collector releases the resources used by grad object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T14:32:11.046547Z",
     "iopub.status.busy": "2020-11-03T14:32:11.045879Z",
     "iopub.status.idle": "2020-11-03T14:32:11.101125Z",
     "shell.execute_reply": "2020-11-03T14:32:11.097081Z",
     "shell.execute_reply.started": "2020-11-03T14:32:11.046465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The df/dx where f=(x)^2: \n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d16a27fa9f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The df/dx where f=(x)^2: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The df/dx where h=(x)^3: \\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/my_bin/apps/miniconda3/envs/tf-gpu-2.2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \"\"\"\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[1;32m   1000\u001b[0m                          \"non-persistent tapes.\")\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as grad:\n",
    "    f = x ** 2\n",
    "    h = x ** 3\n",
    "\n",
    "print('The df/dx where f=(x)^2: \\n', grad.gradient(f, x))\n",
    "print('The df/dx where h=(x)^3: \\n', grad.gradient(h, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
