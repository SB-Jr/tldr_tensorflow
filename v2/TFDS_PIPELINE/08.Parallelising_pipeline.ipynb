{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main focus of the ETL pipeline is to make sure that the GPU and CPU are working in sync and that none of the devices are in idle state and so that we get the most out of the hardware we have. The job of CPU is to do all the ETL process and the GPU's task is take the data from the ETL pipline and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the preprocessing of the data is done by the CPU, it often can become the botlleneck. This can be avoided with a good efficient pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "One method to create a efficient ETL pipeline is to use Data caching. Datasets/Tensors can be cached in 2 ways:\n",
    "- Cachin in memory\n",
    "- Cachine in Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching in memory\n",
    "\n",
    "Here the caching takes place in the RAM. \n",
    "we use `tf.data.Dataset.cache()` to cache the dataset so that the need to do pre-processing again and again for each epoch is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching in Disk\n",
    "\n",
    "Here we use the `tf.data.Dataset.cache(filename='<file name here>')` and can then store the cache on disk if the data is too big to e stored in the memory(RAM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel APIs\n",
    "\n",
    "\n",
    "The main APIs in `tf.data.Dataset` that we can use to take the advantage of parallelism to get the most out of our hardware are:\n",
    "- map\n",
    "- prefetch\n",
    "- interleave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOTUNE\n",
    "\n",
    "Most of the parallel APIs will need us to give values to parameters based on our system configuraion. These values can be hardcoded in most of the casses but in case of Cloud architectures, where the system are scalled all the time eihter horizontally or vertically, with changing system configuration, we cant keep chaging the code to keep providing the hardocded data. \n",
    "\n",
    "This is where the Autotune API helps us. We can set almost all the variables or parameters with Autotune so that the proper values to get the max utilization of the resources are carried by the TF itself.\n",
    "\n",
    "```python\n",
    "from tensorflow.data.experimental import AUTOTUNE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "Most often we need to perform preprocessing like augmentation on the data before it is passed further towards the model. This can be a very expensive taks if we dont utilize all the cores of the CPU to parallelize the process.\n",
    "eg:\n",
    "```python\n",
    "def augmentation(features):\n",
    "    x = tf.image.random_flip_left_right(features['image'])\n",
    "    x = tf.image.random_flip_up_down(x)\n",
    "    x = tf.image.random_brightness(x, max_dedlta=0.1)\n",
    "    x = tf.image.random_saturation(x, lower=0.75, upper=1.5)\n",
    "    x = tf.image.random_hue(x, max_delta=0.15)\n",
    "    x = tf.image.random_contrast(x, lower=0.75, upper=1.5)\n",
    "    x = tf.image.resize(x, (224, 224))\n",
    "    image = x/255.0\n",
    "    return image, features['label']\n",
    "\n",
    "# load dataset\n",
    "dataset = tfds.load(cats_dogs, split=tfds.Split.TRAIN)\n",
    "# how many cores of CPU do u have?\n",
    "cores = 8\n",
    "augmented_dataset = dataset.map(augmentation, num_parallel_calls=cores-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefetch\n",
    "\n",
    "We can use prefetch so that the data preprocessing for the next epoch can be done by the CPU while the epoch is being executed on the GPU. This way the CPU and GPU are being used simultaneouslt at the same time thus increasing the system throughput.\n",
    "\n",
    "eg:\n",
    "```python\n",
    "preped_dataset = dataset.map(augmentation_fn).prefetch(AUTOTUNE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inteleave\n",
    "\n",
    "We can also try to optimize the data extraction so that the data that has been extracted( or loaded into memeory or is ready for further use) can be preprocessed so that the CPU resources are used properly. So basically here we are trying to parallelize the I/O and preprocessing(map) operation.\n",
    "\n",
    "eg:\n",
    "```python\n",
    "files = tf.data.Dataset.list_files('regex to cover all files')\n",
    "\n",
    "num_parallel_reads = 4\n",
    "\n",
    "dataset = files.interleave(\n",
    "    tf.data.TFRecordDataset,  # map function\n",
    "    cycle_length=num_parallel_reads, \n",
    "    num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
