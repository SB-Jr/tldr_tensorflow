{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Artificial-Neural-Network\" data-toc-modified-id=\"Artificial-Neural-Network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Artificial Neural Network</a></span></li><li><span><a href=\"#Loss-function\" data-toc-modified-id=\"Loss-function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loss function</a></span></li><li><span><a href=\"#Optimizer\" data-toc-modified-id=\"Optimizer-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Optimizer</a></span></li><li><span><a href=\"#Weight-Regularization\" data-toc-modified-id=\"Weight-Regularization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Weight Regularization</a></span></li><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Training-a-neural-network\" data-toc-modified-id=\"Training-a-neural-network-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training a neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step1:-import-the-data\" data-toc-modified-id=\"Step1:-import-the-data-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Step1: import the data</a></span></li><li><span><a href=\"#Step2:-Transform-the-data\" data-toc-modified-id=\"Step2:-Transform-the-data-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Step2: Transform the data</a></span></li><li><span><a href=\"#Step3:-Construct-the-tensor\" data-toc-modified-id=\"Step3:-Construct-the-tensor-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Step3: Construct the tensor</a></span></li><li><span><a href=\"#Step4:-Model-creation\" data-toc-modified-id=\"Step4:-Model-creation-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Step4: Model creation</a></span></li><li><span><a href=\"#Step5:-Train-and-evaluate-the-model\" data-toc-modified-id=\"Step5:-Train-and-evaluate-the-model-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>Step5: Train and evaluate the model</a></span></li><li><span><a href=\"#Step6:-Improve-the-model\" data-toc-modified-id=\"Step6:-Improve-the-model-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Step6: Improve the model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/ann.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Features are the input and labels are the ouput.<br>\n",
    "Here ANN is composed of 4 main components:\n",
    "- Layers: all the learning occurs in the layers. There are 3 layers 1) Input 2) Hidden and 3) Output\n",
    "- feature and label: Input data to the network(features) and output from the network (labels)\n",
    "- loss function: Metric used to estimate the performance of the learning phase\n",
    "- optimizer: Improve the learning by updating the knowledge in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer consists of neurons or nodes. Each neuron has 2 parts:\n",
    "- input\n",
    "- activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "After you have defined the hidden layers and the activation function, you need to specify the loss function and the optimizer.\n",
    "\n",
    "For binary classification, it is common practice to use a binary cross entropy loss function. In the linear regression, you use the mean square error.\n",
    "\n",
    "The loss function is an important metric to estimate the performance of the optimizer. During the training, this metric will be minimized. You need to select this quantity carefully depending on the type of problem you are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "The loss function is a measure of the model's performance. The optimizer will help improve the weights of the network in order to decrease the loss. There are different optimizers available, but the most common one is the Stochastic Gradient Descent.\n",
    "\n",
    "The conventional optimizers are:\n",
    "- Momentum optimization,\n",
    "- Nesterov Accelerated Gradient,\n",
    "- AdaGrad,\n",
    "- Adam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Regularization\n",
    "A standard technique to prevent overfitting is to add constraints to the weights of the network. The constraint forces the size of the network to take only small values. The constraint is added to the loss function of the error. There are two kinds of regularization:\n",
    "\n",
    "L1: Lasso: Cost is proportional to the absolute value of the weight coefficients\n",
    "\n",
    "L2: Ridge: Cost is proportional to the square of the value of the weight coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "Dropout is an odd but useful technique. A network with dropout means that some weights will be randomly set to zero. Imagine you have an array of weights [0.1, 1.7, 0.7, -0.9]. If the neural network has a dropout, it will become [0.1, 0, 0, -0.9] with randomly distributed 0. The parameter that controls the dropout is the dropout rate. The rate defines how many weights to be set to zeroes. Having a rate between 0.2 and 0.5 is common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: import the data\n",
    "\n",
    "The MNIST dataset is the commonly used dataset to test new techniques or algorithms. This dataset is a collection of 28x28 pixel image with a handwritten digit from 0 to 9. Currently, the lowest error on the test is 0.27 percent with a committee of 7 convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:02:55.591192Z",
     "start_time": "2020-02-19T15:02:53.609065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tf.disable_eager_execution()\n",
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:03:14.785629Z",
     "start_time": "2020-02-19T15:02:56.807703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784', data_home='data/mnist')\n",
    "print(mnist.data.shape)\n",
    "print(mnist.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:03:16.698185Z",
     "start_time": "2020-02-19T15:03:15.895776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56000, 784)\n",
      "(14000, 784)\n",
      "(56000,)\n",
      "(14000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist.data, mnist.target, test_size = 0.2, random_state =42)\n",
    "print(X_train.shape)  # 56000 examples of 28x28=784 pixel data\n",
    "print(X_test.shape)   # 14000 examples of 28*28=784 pixel data\n",
    "Y_train = Y_train.astype(int)\n",
    "Y_test = Y_test.astype(int)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2: Transform the data\n",
    "\n",
    "we will use min max transformation<br>\n",
    "$\\frac{X- X_{min}}{X_{max} - X_{min}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:03:20.023023Z",
     "start_time": "2020-02-19T15:03:19.295557Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "X_test_scaled = scaler.fit_transform(X_test.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Construct the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:04:00.738114Z",
     "start_time": "2020-02-19T15:04:00.729091Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_column = [tf.feature_column.numeric_column('X', shape=X_train_scaled.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:06:02.291170Z",
     "start_time": "2020-02-19T15:06:02.061062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'logs/11_ann', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9754b5df90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.DNNClassifier(feature_columns=feature_column,\n",
    "                                  hidden_units= [300, 100],\n",
    "                                  n_classes=10,\n",
    "                                  model_dir='logs/11_ann')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5: Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:14:10.797160Z",
     "start_time": "2020-02-19T15:14:10.787656Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_input_fn(x_ndarray, y_nd_array, num_epochs = None, batch_size = 128, shufle=False):\n",
    "    return tf.estimator.inputs.numpy_input_fn(x = {'X': x_ndarray},\n",
    "                                             y = y_nd_array,\n",
    "                                             batch_size = batch_size,\n",
    "                                             num_epochs=num_epochs,\n",
    "                                             shuffle=shufle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:14:25.956114Z",
     "start_time": "2020-02-19T15:14:15.514589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /home/sbjr/my_bin/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/sbjr/my_bin/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/sbjr/my_bin/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py:882: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into logs/11_ann/model.ckpt.\n",
      "INFO:tensorflow:loss = 302.03247, step = 0\n",
      "INFO:tensorflow:global_step/sec: 138.782\n",
      "INFO:tensorflow:loss = 36.20991, step = 100 (0.724 sec)\n",
      "INFO:tensorflow:global_step/sec: 109.578\n",
      "INFO:tensorflow:loss = 9.240826, step = 200 (0.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 110.101\n",
      "INFO:tensorflow:loss = 16.542458, step = 300 (0.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 105.864\n",
      "INFO:tensorflow:loss = 21.090855, step = 400 (0.945 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.103\n",
      "INFO:tensorflow:loss = 7.868234, step = 500 (0.861 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.142\n",
      "INFO:tensorflow:loss = 17.605541, step = 600 (0.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 118.396\n",
      "INFO:tensorflow:loss = 10.778568, step = 700 (0.845 sec)\n",
      "INFO:tensorflow:global_step/sec: 117.483\n",
      "INFO:tensorflow:loss = 5.0709305, step = 800 (0.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 100.423\n",
      "INFO:tensorflow:loss = 13.556429, step = 900 (0.996 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into logs/11_ann/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.014925.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f975440e610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf logs/11_ann\n",
    "model.train(input_fn=get_input_fn(X_train_scaled, Y_train, None, 128, False), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T15:21:25.237671Z",
     "start_time": "2020-02-19T15:21:24.220070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-02-19T20:51:24Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from logs/11_ann/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2020-02-19-20:51:25\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9694286, average_loss = 0.10564723, global_step = 1000, loss = 13.446011\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: logs/11_ann/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9694286,\n",
       " 'average_loss': 0.10564723,\n",
       " 'loss': 13.446011,\n",
       " 'global_step': 1000}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(input_fn=get_input_fn(X_test_scaled, Y_test, 1, 128, False), steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6: Improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use regularization to improve the model.<br>\n",
    "We will use an Adam optimizer with a dropout rate of 0.3, L1 of X and L2 of y. In TensorFlow, you can control the optimizer using the object train following by the name of the optimizer. TensorFlow is a built-in API for Proximal AdaGrad optimizer.\n",
    "\n",
    "To add regularization to the deep neural network, you can use tf.train.ProximalAdagradOptimizer with the following parameter\n",
    "\n",
    "- Learning rate: learning_rate\n",
    "- L1 regularization: l1_regularization_strength\n",
    "- L2 regularization: l2_regularization_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T16:26:05.342467Z",
     "start_time": "2020-02-19T16:26:05.327728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'logs/11_ann_improved', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f972c2bf990>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_improved = tf.estimator.DNNClassifier(feature_columns=feature_column,\n",
    "                                           hidden_units=[300,100],\n",
    "                                           dropout=0.3,\n",
    "                                           n_classes=10,\n",
    "                                           optimizer= tf.train.ProximalAdagradOptimizer(learning_rate=0.01,\n",
    "                                                                                       l1_regularization_strength=0.01,\n",
    "                                                                                       l2_regularization_strength=0.01),\n",
    "                                           model_dir='logs/11_ann_improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T16:26:56.052704Z",
     "start_time": "2020-02-19T16:26:50.710564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into logs/11_ann_improved/model.ckpt.\n",
      "INFO:tensorflow:loss = 303.69342, step = 0\n",
      "INFO:tensorflow:global_step/sec: 200.584\n",
      "INFO:tensorflow:loss = 46.474987, step = 100 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 261.891\n",
      "INFO:tensorflow:loss = 15.754684, step = 200 (0.382 sec)\n",
      "INFO:tensorflow:global_step/sec: 230.001\n",
      "INFO:tensorflow:loss = 20.9094, step = 300 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 235.361\n",
      "INFO:tensorflow:loss = 25.11753, step = 400 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 228.7\n",
      "INFO:tensorflow:loss = 24.159872, step = 500 (0.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.231\n",
      "INFO:tensorflow:loss = 36.00038, step = 600 (0.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 259.664\n",
      "INFO:tensorflow:loss = 21.821949, step = 700 (0.385 sec)\n",
      "INFO:tensorflow:global_step/sec: 255.672\n",
      "INFO:tensorflow:loss = 13.795448, step = 800 (0.391 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.262\n",
      "INFO:tensorflow:loss = 29.585947, step = 900 (0.480 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into logs/11_ann_improved/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 22.30181.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f972c2b7710>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf logs/11_ann_improved\n",
    "model_improved.train(input_fn=get_input_fn(X_train_scaled, Y_train, None, 128, False), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T16:27:27.197361Z",
     "start_time": "2020-02-19T16:27:25.603787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-02-19T21:57:25Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from logs/11_ann_improved/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [100/1000]\n",
      "INFO:tensorflow:Finished evaluation at 2020-02-19-21:57:27\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.9582857, average_loss = 0.14352892, global_step = 1000, loss = 18.267319\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: logs/11_ann_improved/model.ckpt-1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9582857,\n",
       " 'average_loss': 0.14352892,\n",
       " 'loss': 18.267319,\n",
       " 'global_step': 1000}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_improved.evaluate(input_fn=get_input_fn(X_test_scaled, Y_test, 1, 128, False), steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
