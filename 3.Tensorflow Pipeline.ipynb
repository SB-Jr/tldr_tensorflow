{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Pipeline\n",
    "\n",
    "The pipeline will load the data in batch, or small chunk. Each batch will be pushed to the pipeline and be ready for the training. Building a pipeline is an excellent solution because it allows you to use parallel computing. It means Tensorflow will train the model across multiple CPUs. It fosters the computation and permits for training powerful neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to create a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:56:59.217869Z",
     "start_time": "2020-02-04T11:56:59.208239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.62460588 0.7553646  0.88575139 0.55813575]\n",
      " [0.47794668 0.94377006 0.63552177 0.56810112]\n",
      " [0.86221862 0.73088896 0.36357698 0.73940885]]\n"
     ]
    }
   ],
   "source": [
    "#here we will use numpy to generate arbitary data\n",
    "import numpy as np\n",
    "\n",
    "x_input = np.random.sample((3,4)) #data dimension is 3x4\n",
    "print(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholders\n",
    "\n",
    "create the place holders to hold the data while running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:56:52.569135Z",
     "start_time": "2020-02-04T11:56:51.197181Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset\n",
    "\n",
    "<b>Note:It is strongly advised to not use feed_dict to provide data while running a session.</b><br>\n",
    "\n",
    "This is where `dataset` and `iterator` come in picture.Datasets can be generated using multiple type of data sources like Numpy, TFRecords, text files, CSV files etc.<br>\n",
    "\n",
    "\n",
    "Dataset can be created in multiple ways:\n",
    "<ul>\n",
    "    <li><b><u>.from_tensor_slices()</u></b>: This method accepts individual (or multiple) Numpy (or Tensors) objects.In case you are feeding multiple objects, pass them as tuple and make sure that all the objects have same size in zeroth dimension.</li>\n",
    "    <li><b><u>.from_tensors()</u></b>: Just like from_tensor_slices, this method also accepts individual (or multiple) Numpy (or Tensors) objects. But this method doesn’t support batching of data, i.e all the data will be given out instantly. As a result, you can pass differently sized inputs at zeroth dimension if you are passing multiple objects. This method is useful in cases where dataset is very small or your learning model needs all the data at once.</li>\n",
    "    <li><b><u>.from_generator()</u></b>: In this method, a generator function is passed as input. This method is useful in cases where you wish to generate the data at runtime and as such no raw data exists with you or in scenarios where your training data is extremely huge and it is not possible to store them in your disk. I would strongly encourage people to <b>not use</b> this method for the purpose of generating data augmentations.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Once the dataset has been created we can apply different kinds of transformations like batch, repeat, shuffle, map or filter.</p>\n",
    "\n",
    "\n",
    "We need to define the Dataset where we can populate the value of the placeholder x. We need to use the method `tf.data.Dataset.from_tensor_slices`<br>\n",
    "<b>from_tensor_slices</b>: This method accepts individual (or multiple) Numpy (or Tensors) objects. In case you are feeding multiple objects, pass them as tuple and make sure that all the objects have same size in zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:57:02.146284Z",
     "start_time": "2020-02-04T11:57:02.138258Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the pipeline\n",
    "\n",
    "We need to initialize the pipeline where the data will flow. We need to create an iterator with `make_initializable_iterator`. We name it iterator. Then we need to call this iterator to feed the next batch of data, `get_next`. We name this step get_next. Note that in our example, there is only one batch of data<br>\n",
    "\n",
    "Tensorflow has provided four types of iterators and each of them has a specific purpose and use-case behind it.\n",
    "<ul>\n",
    "    <li><b><u>one_shot_iterator</u></b>:One-shot iterator will iterate through all the elements present in Dataset and once exhausted, cannot be used anymore.</li>\n",
    "    <li><b><u>initializable</u></b>:In One-shot iterator, we had the shortfall of repetition of same training dataset in memory and there was absence of periodically validating our model using validation dataset in our code. In initializable iterator we overcome these problems. Initializable iterator has to be initialized with dataset before it starts running.</li>\n",
    "    <li><b><u>reinitializable</u></b>:In initializable iterator, there was a shortfall of different datasets undergoing the same pipeline before the Dataset is fed into the iterator. This problem is overcome by reinitializable iterator as we have the ability to feed different types of Datasets thereby undergoing different pipelines. Only one care has to be taken is that different Datasets are of the same data type.</li>\n",
    "    <li><b><u>feedable</u></b>:The reinitializable iterator gave the flexibility of assigning differently pipelined Datasets to iterator, but the iterator was inadequate to maintain the state (i.e till where the data has been emitted by individual iterator).</li>\n",
    "</ul>\n",
    "\n",
    "Regardless of the type of iterator, `get_next` function of iterator is used to create an operation in your Tensorflow graph which when run over a session, returns the values from the fed Dataset of iterator. Also, iterator doesn’t keep track of how many elements are present in the Dataset. Hence, it is normal to keep running the iterator’s get_next operation till Tensorflow’s `tf.errors.OutOfRangeError` exception is occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:57:05.080292Z",
     "start_time": "2020-02-04T11:57:05.060181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:0\", shape=(4,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.data.make_initializable_iterator(dataset)\n",
    "get_next = iterator.get_next()\n",
    "print(get_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Operation\n",
    "\n",
    "We initiate a session, and we run the operation iterator. We feed the feed_dict with the value generated by numpy. These two value will populate the placeholder x. Then we run get_next to print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T11:57:07.282024Z",
     "start_time": "2020-02-04T11:57:07.167883Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62460588 0.7553646  0.88575139 0.55813575]\n",
      "[0.47794668 0.94377006 0.63552177 0.56810112]\n",
      "[0.86221862 0.73088896 0.36357698 0.73940885]\n",
      "---Finished Execution---\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    try:\n",
    "        while True:\n",
    "            print(sess.run(get_next))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('---Finished Execution---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
